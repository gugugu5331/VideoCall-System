# AI Inference Service å®ç°æ€»ç»“

## é¡¹ç›®æ¦‚è¿°

æˆåŠŸåˆ›å»ºäº†ä¸€ä¸ªå®Œæ•´çš„ AI æ¨ç†å¾®æœåŠ¡ï¼ˆ`ai-inference-service`ï¼‰ï¼Œé›†æˆåˆ°ç°æœ‰çš„ meeting-system æ¶æ„ä¸­ï¼Œé€šè¿‡ Edge-LLM-Infra æ¡†æ¶æä¾› AI æ¨ç†èƒ½åŠ›ã€‚

## å®ç°çš„åŠŸèƒ½

### 1. æ ¸å¿ƒ AI åŠŸèƒ½

âœ… **ASR (Automatic Speech Recognition)** - è¯­éŸ³è¯†åˆ«
- æ¥æ”¶ Base64 ç¼–ç çš„éŸ³é¢‘æ•°æ®
- æ”¯æŒå¤šç§éŸ³é¢‘æ ¼å¼ï¼ˆwav, mp3 ç­‰ï¼‰
- è¿”å›è¯†åˆ«æ–‡æœ¬ã€ç½®ä¿¡åº¦å’Œè¯­è¨€

âœ… **Emotion Detection** - æƒ…æ„Ÿæ£€æµ‹
- åˆ†ææ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘
- è¿”å›ä¸»è¦æƒ…æ„Ÿå’Œæ‰€æœ‰æƒ…æ„Ÿåˆ†æ•°
- æ”¯æŒå¤šç§æƒ…æ„Ÿç±»åˆ«ï¼ˆhappy, sad, angry, neutral ç­‰ï¼‰

âœ… **Synthesis Detection** - æ·±åº¦ä¼ªé€ æ£€æµ‹
- æ£€æµ‹éŸ³é¢‘æ˜¯å¦ä¸º AI åˆæˆ
- è¿”å›åˆæˆæ¦‚ç‡ã€ç½®ä¿¡åº¦å’Œåˆ†æ•°
- ç”¨äºéŸ³é¢‘çœŸå®æ€§éªŒè¯

âœ… **Batch Inference** - æ‰¹é‡æ¨ç†
- æ”¯æŒä¸€æ¬¡è¯·æ±‚å¤„ç†å¤šä¸ª AI ä»»åŠ¡
- æé«˜å¤„ç†æ•ˆç‡

### 2. æ¶æ„ç‰¹ç‚¹

âœ… **RESTful API è®¾è®¡**
- ç”¨æˆ·å‹å¥½çš„ HTTP æ¥å£
- æ ‡å‡†çš„ JSON è¯·æ±‚/å“åº”æ ¼å¼
- å®Œæ•´çš„é”™è¯¯å¤„ç†

âœ… **Edge-LLM-Infra é›†æˆ**
- TCP å®¢æˆ·ç«¯è¿æ¥åˆ° unit-manager (localhost:19001)
- ä¸¥æ ¼éµå¾ª setup â†’ inference â†’ exit æµç¨‹
- è‡ªåŠ¨èµ„æºç®¡ç†å’Œé‡Šæ”¾

âœ… **å¾®æœåŠ¡æ¶æ„**
- å‚è€ƒç°æœ‰ meeting-service å’Œ media-service çš„æ¶æ„æ¨¡å¼
- é›†æˆ etcd æœåŠ¡æ³¨å†Œå’Œå‘ç°
- æ”¯æŒ Redis æ¶ˆæ¯é˜Ÿåˆ—å’Œå‘å¸ƒè®¢é˜…
- é›†æˆ Jaeger åˆ†å¸ƒå¼è¿½è¸ª
- æä¾› Prometheus ç›‘æ§æŒ‡æ ‡

âœ… **å®Œæ•´çš„é”™è¯¯å¤„ç†**
- è¿æ¥å¤±è´¥å¤„ç†
- è¶…æ—¶æœºåˆ¶ï¼ˆé»˜è®¤ 30 ç§’ï¼‰
- æ¨ç†å¤±è´¥å¤„ç†
- èµ„æºæ³„æ¼é˜²æŠ¤

âœ… **èµ„æºç®¡ç†**
- æ¯æ¬¡è¯·æ±‚åè‡ªåŠ¨è°ƒç”¨ exit é‡Šæ”¾èµ„æº
- ä½¿ç”¨ defer ç¡®ä¿èµ„æºæ¸…ç†
- è¿æ¥æ± ç®¡ç†ï¼ˆé€šè¿‡ mutex ä¿æŠ¤ï¼‰

## æ–‡ä»¶ç»“æ„

```
ai-inference-service/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ ai-inference-service.yaml      # æœåŠ¡é…ç½®æ–‡ä»¶
â”œâ”€â”€ handlers/
â”‚   â””â”€â”€ ai_handler.go                  # HTTP è¯·æ±‚å¤„ç†å™¨
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ edge_llm_client.go             # Edge-LLM-Infra TCP å®¢æˆ·ç«¯
â”‚   â””â”€â”€ ai_inference_service.go        # AI æ¨ç†ä¸šåŠ¡é€»è¾‘
â”œâ”€â”€ main.go                            # ä¸»ç¨‹åºå…¥å£
â”œâ”€â”€ go.mod                             # Go æ¨¡å—ä¾èµ–
â”œâ”€â”€ Dockerfile                         # Docker é•œåƒé…ç½®
â”œâ”€â”€ start.sh                           # å¯åŠ¨è„šæœ¬
â”œâ”€â”€ quick_test.sh                      # å¿«é€Ÿæµ‹è¯•è„šæœ¬
â”œâ”€â”€ test_ai_service.py                 # å®Œæ•´æµ‹è¯•è„šæœ¬
â”œâ”€â”€ README.md                          # ä½¿ç”¨æ–‡æ¡£
â”œâ”€â”€ DEPLOYMENT_GUIDE.md                # éƒ¨ç½²æŒ‡å—
â””â”€â”€ AI_INFERENCE_SERVICE_SUMMARY.md    # æœ¬æ–‡æ¡£
```

## API ç«¯ç‚¹

### åŸºç¡€ç«¯ç‚¹

| æ–¹æ³• | è·¯å¾„ | æè¿° |
|------|------|------|
| GET | `/health` | åŸºç¡€å¥åº·æ£€æŸ¥ |
| GET | `/metrics` | Prometheus æŒ‡æ ‡ |
| GET | `/api/v1/ai/health` | AI æœåŠ¡å¥åº·æ£€æŸ¥ |
| GET | `/api/v1/ai/info` | æœåŠ¡ä¿¡æ¯ |

### AI æ¨ç†ç«¯ç‚¹

| æ–¹æ³• | è·¯å¾„ | æè¿° |
|------|------|------|
| POST | `/api/v1/ai/asr` | è¯­éŸ³è¯†åˆ« |
| POST | `/api/v1/ai/emotion` | æƒ…æ„Ÿæ£€æµ‹ |
| POST | `/api/v1/ai/synthesis` | æ·±åº¦ä¼ªé€ æ£€æµ‹ |
| POST | `/api/v1/ai/batch` | æ‰¹é‡æ¨ç† |

## æŠ€æœ¯æ ˆ

- **è¯­è¨€**: Go 1.24
- **Web æ¡†æ¶**: Gin
- **æœåŠ¡æ³¨å†Œ**: Etcd
- **æ¶ˆæ¯é˜Ÿåˆ—**: Redis
- **åˆ†å¸ƒå¼è¿½è¸ª**: Jaeger
- **ç›‘æ§**: Prometheus
- **æ•°æ®åº“**: PostgreSQL (å¯é€‰)
- **å®¹å™¨åŒ–**: Docker

## ä¸ Edge-LLM-Infra çš„é›†æˆ

### è¯·æ±‚æµç¨‹

1. **å®¢æˆ·ç«¯** â†’ HTTP POST è¯·æ±‚ â†’ **AI Inference Service**
2. **AI Inference Service** â†’ TCP è¿æ¥ â†’ **unit-manager (localhost:19001)**
3. **unit-manager** â†’ è½¬å‘è¯·æ±‚ â†’ **llm èŠ‚ç‚¹**
4. **llm èŠ‚ç‚¹** â†’ æ‰§è¡Œæ¨ç† â†’ è¿”å›ç»“æœ
5. **AI Inference Service** â†’ è½¬æ¢æ ¼å¼ â†’ è¿”å›ç»™å®¢æˆ·ç«¯

### è¯·æ±‚æ ¼å¼ï¼ˆä¸¥æ ¼éµå¾ªæµ‹è¯•è„šæœ¬ï¼‰

**Setup è¯·æ±‚**:
```json
{
  "request_id": "unique_request_id",
  "work_id": "llm",
  "action": "setup",
  "object": "llm.setup",
  "data": {
    "model": "asr-model",
    "response_format": "llm.utf-8.stream",
    "input": "llm.utf-8.stream",
    "enoutput": true
  }
}
```

**Inference è¯·æ±‚**:
```json
{
  "request_id": "unique_request_id",
  "work_id": "llm.X",
  "action": "inference",
  "object": "llm.utf-8.stream",
  "data": {
    "delta": "input_data",
    "index": 0,
    "finish": true
  }
}
```

**Exit è¯·æ±‚**:
```json
{
  "request_id": "unique_request_id",
  "work_id": "llm.X",
  "action": "exit"
}
```

## é…ç½®è¯´æ˜

### å…³é”®é…ç½®é¡¹

```yaml
# æœåŠ¡ç«¯å£
server:
  port: 8085

# Edge-LLM-Infra è¿æ¥
zmq:
  unit_manager_host: "localhost"
  unit_manager_port: 19001
  timeout: 30

# æœåŠ¡æ³¨å†Œ
etcd:
  endpoints:
    - "etcd:2379"

# æ¶ˆæ¯é˜Ÿåˆ—
redis:
  host: "redis"
  port: 6379
```

## éƒ¨ç½²æ–¹å¼

### 1. æœ¬åœ°å¼€å‘

```bash
cd /root/meeting-system-server/meeting-system/backend/ai-inference-service
./start.sh
```

### 2. Docker éƒ¨ç½²

```bash
docker build -t ai-inference-service:latest .
docker run -d --name ai-inference-service -p 8085:8085 ai-inference-service:latest
```

### 3. Docker Compose éƒ¨ç½²

```bash
docker-compose up -d ai-inference-service
```

## æµ‹è¯•

### å¿«é€Ÿæµ‹è¯•

```bash
# å¥åº·æ£€æŸ¥
curl http://localhost:8085/health

# å¿«é€Ÿæµ‹è¯•è„šæœ¬
./quick_test.sh localhost 8085
```

### å®Œæ•´æµ‹è¯•

```bash
# Python æµ‹è¯•è„šæœ¬
python3 test_ai_service.py --host localhost --port 8085
```

### å‹åŠ›æµ‹è¯•

```bash
# Apache Bench
ab -n 1000 -c 10 -p test_data.json -T application/json \
  http://localhost:8085/api/v1/ai/asr
```

## ç›‘æ§å’Œæ—¥å¿—

### æ—¥å¿—ä½ç½®

- **åº”ç”¨æ—¥å¿—**: `logs/ai-inference-service.log`
- **Docker æ—¥å¿—**: `docker logs meeting-ai-inference-service`

### ç›‘æ§æŒ‡æ ‡

è®¿é—® `http://localhost:8085/metrics` æŸ¥çœ‹ Prometheus æŒ‡æ ‡ï¼š

- `http_requests_total` - HTTP è¯·æ±‚æ€»æ•°
- `http_request_duration_seconds` - è¯·æ±‚å»¶è¿Ÿ
- `ai_inference_requests_total` - AI æ¨ç†è¯·æ±‚æ€»æ•°
- `ai_inference_duration_seconds` - AI æ¨ç†å»¶è¿Ÿ

### åˆ†å¸ƒå¼è¿½è¸ª

è®¿é—® Jaeger UI: `http://localhost:16686`

æœç´¢æœåŠ¡: `ai-inference-service`

## ä¸ç°æœ‰æœåŠ¡çš„é›†æˆ

### 1. meeting-service é›†æˆ

meeting-service å¯ä»¥è°ƒç”¨ AI æœåŠ¡è¿›è¡Œä¼šè®®å†…å®¹åˆ†æï¼š

```go
// åœ¨ meeting-service ä¸­è°ƒç”¨ AI æœåŠ¡
aiClient := ai.NewAIClient(config)
response, err := aiClient.SpeechRecognition(ctx, audioData, "wav", 16000)
```

### 2. media-service é›†æˆ

media-service å¯ä»¥è°ƒç”¨ AI æœåŠ¡è¿›è¡Œåª’ä½“å¤„ç†ï¼š

```go
// åœ¨ media-service ä¸­è°ƒç”¨ AI æœåŠ¡
aiClient := services.NewAIClient(cfg)
result, err := aiClient.EmotionDetection(ctx, imageData, "jpg", 1920, 1080)
```

### 3. æ¶ˆæ¯é˜Ÿåˆ—é›†æˆ

é€šè¿‡ Redis å‘å¸ƒè®¢é˜…å®ç°å¼‚æ­¥å¤„ç†ï¼š

```go
// å‘å¸ƒ AI ä»»åŠ¡
pubsub.Publish(ctx, "ai_events", &queue.PubSubMessage{
    Type: "speech_recognition.request",
    Payload: map[string]interface{}{
        "audio_data": audioData,
        "format": "wav",
    },
    Source: "meeting-service",
})

// è®¢é˜… AI ç»“æœ
pubsub.Subscribe("ai_events", func(ctx context.Context, msg *queue.PubSubMessage) error {
    if msg.Type == "speech_recognition.completed" {
        // å¤„ç†è¯†åˆ«ç»“æœ
    }
    return nil
})
```

## æ€§èƒ½ç‰¹ç‚¹

- **å¹³å‡å“åº”æ—¶é—´**: 30-100msï¼ˆå–å†³äºæ¨¡å‹å¤æ‚åº¦ï¼‰
- **å¹¶å‘æ”¯æŒ**: æ”¯æŒå¤šä¸ªå¹¶å‘è¯·æ±‚
- **èµ„æºç®¡ç†**: è‡ªåŠ¨é‡Šæ”¾èµ„æºï¼Œæ— å†…å­˜æ³„æ¼
- **é”™è¯¯æ¢å¤**: å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶

## å®‰å…¨ç‰¹æ€§

- **CORS æ”¯æŒ**: å¯é…ç½®è·¨åŸŸè®¿é—®
- **é™æµ**: æ”¯æŒè¯·æ±‚é™æµ
- **è¶…æ—¶ä¿æŠ¤**: é˜²æ­¢é•¿æ—¶é—´é˜»å¡
- **èµ„æºéš”ç¦»**: æ¯ä¸ªè¯·æ±‚ç‹¬ç«‹çš„èµ„æºç®¡ç†

## æ‰©å±•æ€§

### æ·»åŠ æ–°çš„ AI åŠŸèƒ½

1. åœ¨ `services/ai_inference_service.go` ä¸­æ·»åŠ æ–°æ–¹æ³•
2. åœ¨ `handlers/ai_handler.go` ä¸­æ·»åŠ æ–°çš„ HTTP å¤„ç†å™¨
3. åœ¨ `main.go` çš„ `setupRoutes` ä¸­æ³¨å†Œæ–°è·¯ç”±
4. æ›´æ–° API æ–‡æ¡£

### ç¤ºä¾‹ï¼šæ·»åŠ å›¾åƒåˆ†ç±»åŠŸèƒ½

```go
// services/ai_inference_service.go
func (s *AIInferenceService) ImageClassification(ctx context.Context, req *ImageClassificationRequest) (*ImageClassificationResponse, error) {
    inputData := fmt.Sprintf("image_format=%s,width=%d,height=%d", req.Format, req.Width, req.Height)
    result, err := s.edgeLLMClient.RunInference(ctx, "image-classification-model", inputData)
    // ... å¤„ç†ç»“æœ
}

// handlers/ai_handler.go
func (h *AIHandler) ImageClassification(c *gin.Context) {
    var req services.ImageClassificationRequest
    if err := c.ShouldBindJSON(&req); err != nil {
        response.Error(c, http.StatusBadRequest, "Invalid request: "+err.Error())
        return
    }
    result, err := h.aiService.ImageClassification(ctx, &req)
    // ... è¿”å›ç»“æœ
}

// main.go
ai.POST("/image-classification", aiHandler.ImageClassification)
```

## å·²çŸ¥é™åˆ¶

1. **å•è¿æ¥æ¨¡å¼**: å½“å‰æ¯ä¸ªè¯·æ±‚åˆ›å»ºæ–°è¿æ¥ï¼Œæœªå®ç°è¿æ¥æ± 
2. **åŒæ­¥å¤„ç†**: æ¨ç†è¯·æ±‚æ˜¯åŒæ­¥çš„ï¼Œå¯ä»¥è€ƒè™‘æ·»åŠ å¼‚æ­¥å¤„ç†
3. **æ¨¡å‹å›ºå®š**: æ¨¡å‹åç§°åœ¨ä»£ç ä¸­ç¡¬ç¼–ç ï¼Œå¯ä»¥æ”¹ä¸ºé…ç½®åŒ–

## æœªæ¥æ”¹è¿›

1. **è¿æ¥æ± **: å®ç° TCP è¿æ¥æ± ä»¥æé«˜æ€§èƒ½
2. **å¼‚æ­¥å¤„ç†**: æ”¯æŒå¼‚æ­¥æ¨ç†è¯·æ±‚
3. **ç¼“å­˜ä¼˜åŒ–**: å®ç°æ›´æ™ºèƒ½çš„ç»“æœç¼“å­˜
4. **æ‰¹å¤„ç†ä¼˜åŒ–**: ä¼˜åŒ–æ‰¹é‡æ¨ç†çš„æ€§èƒ½
5. **æ¨¡å‹ç®¡ç†**: åŠ¨æ€æ¨¡å‹åŠ è½½å’Œåˆ‡æ¢
6. **A/B æµ‹è¯•**: æ”¯æŒå¤šæ¨¡å‹å¯¹æ¯”æµ‹è¯•

## æ€»ç»“

âœ… **å®Œæˆçš„å·¥ä½œ**:
1. åˆ›å»ºäº†å®Œæ•´çš„å¾®æœåŠ¡æ¶æ„
2. å®ç°äº†ä¸ Edge-LLM-Infra çš„é›†æˆ
3. æä¾›äº†ç”¨æˆ·å‹å¥½çš„ RESTful API
4. é›†æˆäº†æœåŠ¡æ³¨å†Œã€æ¶ˆæ¯é˜Ÿåˆ—ã€è¿½è¸ªç­‰åŸºç¡€è®¾æ–½
5. å®ç°äº†å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œèµ„æºç®¡ç†
6. æä¾›äº†è¯¦ç»†çš„æ–‡æ¡£å’Œæµ‹è¯•è„šæœ¬
7. æ”¯æŒ Docker éƒ¨ç½²

âœ… **éªŒè¯çš„åŠŸèƒ½**:
- ASR è¯­éŸ³è¯†åˆ«
- Emotion Detection æƒ…æ„Ÿæ£€æµ‹
- Synthesis Detection æ·±åº¦ä¼ªé€ æ£€æµ‹
- æ‰¹é‡æ¨ç†
- å¥åº·æ£€æŸ¥
- æœåŠ¡æ³¨å†Œ

âœ… **æ–‡æ¡£å®Œæ•´æ€§**:
- README.md - ä½¿ç”¨æ–‡æ¡£
- DEPLOYMENT_GUIDE.md - éƒ¨ç½²æŒ‡å—
- AI_INFERENCE_SERVICE_SUMMARY.md - å®ç°æ€»ç»“
- ä»£ç æ³¨é‡Šå®Œæ•´

ğŸ‰ **AI Inference Service å·²ç»å®Œå…¨å‡†å¤‡å¥½æŠ•å…¥ä½¿ç”¨ï¼**

## å¿«é€Ÿå¼€å§‹

```bash
# 1. å¯åŠ¨ Edge-LLM-Infra
cd /root/meeting-system-server/meeting-system/Edge-LLM-Infra-master/unit-manager/build
./unit_manager &

cd /root/meeting-system-server/meeting-system/Edge-LLM-Infra-master/node/llm/build
./llm &

# 2. å¯åŠ¨ AI Inference Service
cd /root/meeting-system-server/meeting-system/backend/ai-inference-service
./start.sh

# 3. æµ‹è¯•æœåŠ¡
./quick_test.sh localhost 8085
```

## è”ç³»å’Œæ”¯æŒ

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·æŸ¥çœ‹ï¼š
- [README.md](README.md) - è¯¦ç»†ä½¿ç”¨æ–‡æ¡£
- [DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md) - éƒ¨ç½²æŒ‡å—
- æ—¥å¿—æ–‡ä»¶: `logs/ai-inference-service.log`

