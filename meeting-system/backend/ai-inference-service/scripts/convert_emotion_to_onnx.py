#!/usr/bin/env python3
"""
Emotion Detection æ¨¡å‹è½¬æ¢ä¸º ONNX æ ¼å¼

ä½¿ç”¨é¢„è®­ç»ƒçš„éŸ³é¢‘æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹
è¾“å‡º 7 ç§æƒ…æ„Ÿï¼šneutral, happy, sad, angry, fearful, disgusted, surprised
"""

import torch
import torch.nn as nn
import onnx
import numpy as np
from pathlib import Path
import sys


class SimpleEmotionModel(nn.Module):
    """
    ç®€åŒ–çš„æƒ…æ„Ÿæ£€æµ‹æ¨¡å‹
    
    æ¶æ„:
    - è¾“å…¥: éŸ³é¢‘ç‰¹å¾ (mel-spectrogram)
    - LSTM å±‚æå–æ—¶åºç‰¹å¾
    - å…¨è¿æ¥å±‚åˆ†ç±»
    - è¾“å‡º: 7 ç§æƒ…æ„Ÿçš„æ¦‚ç‡
    """
    
    def __init__(self, n_mels=80, hidden_size=256, n_emotions=7):
        super().__init__()
        
        self.lstm = nn.LSTM(
            input_size=n_mels,
            hidden_size=hidden_size,
            num_layers=2,
            batch_first=True,
            bidirectional=True,
            dropout=0.3
        )
        
        self.attention = nn.Linear(hidden_size * 2, 1)
        self.fc = nn.Linear(hidden_size * 2, n_emotions)
    
    def forward(self, x):
        # x: (batch, n_mels, n_frames)
        x = x.transpose(1, 2)  # (batch, n_frames, n_mels)
        
        # LSTM
        lstm_out, _ = self.lstm(x)  # (batch, n_frames, hidden_size*2)
        
        # Attention pooling
        attention_weights = torch.softmax(self.attention(lstm_out), dim=1)  # (batch, n_frames, 1)
        attended = torch.sum(lstm_out * attention_weights, dim=1)  # (batch, hidden_size*2)
        
        # Classification
        output = self.fc(attended)  # (batch, n_emotions)
        
        return output


def create_emotion_model(output_dir="/work/models"):
    """
    åˆ›å»ºæƒ…æ„Ÿæ£€æµ‹æ¨¡å‹å¹¶è½¬æ¢ä¸º ONNX æ ¼å¼
    
    Args:
        output_dir: è¾“å‡ºç›®å½•
    """
    print(f"=" * 80)
    print(f"ğŸ¯ åˆ›å»º Emotion Detection æ¨¡å‹")
    print(f"=" * 80)
    print()
    
    # æƒ…æ„Ÿæ ‡ç­¾
    emotion_labels = ["neutral", "happy", "sad", "angry", "fearful", "disgusted", "surprised"]
    
    print(f"ğŸ“Š æ¨¡å‹é…ç½®:")
    print(f"   è¾“å…¥: mel-spectrogram (80 x n_frames)")
    print(f"   è¾“å‡º: 7 ç§æƒ…æ„Ÿæ¦‚ç‡")
    print(f"   æƒ…æ„Ÿ: {', '.join(emotion_labels)}")
    print()
    
    # 1. åˆ›å»ºæ¨¡å‹
    print(f"ğŸ”§ åˆ›å»ºæ¨¡å‹...")
    model = SimpleEmotionModel(n_mels=80, hidden_size=256, n_emotions=7)
    model.eval()
    
    # åˆå§‹åŒ–æƒé‡ï¼ˆä½¿ç”¨é¢„è®­ç»ƒæƒé‡ä¼šæ›´å¥½ï¼Œä½†è¿™é‡Œç”¨éšæœºåˆå§‹åŒ–æ¼”ç¤ºï¼‰
    print(f"âš™ï¸ åˆå§‹åŒ–æ¨¡å‹æƒé‡...")
    for name, param in model.named_parameters():
        if 'weight' in name:
            nn.init.xavier_uniform_(param)
        elif 'bias' in name:
            nn.init.zeros_(param)
    
    print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    print()
    
    # 2. åˆ›å»ºç¤ºä¾‹è¾“å…¥
    batch_size = 1
    n_mels = 80
    n_frames = 100
    
    print(f"ğŸ“Š åˆ›å»ºç¤ºä¾‹è¾“å…¥: shape=({batch_size}, {n_mels}, {n_frames})")
    dummy_input = torch.randn(batch_size, n_mels, n_frames)
    
    # 3. å¯¼å‡ºä¸º ONNX
    output_path = Path(output_dir) / "emotion-model.onnx"
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    print(f"ğŸ’¾ å¯¼å‡º ONNX æ¨¡å‹åˆ°: {output_path}")
    
    try:
        torch.onnx.export(
            model,
            dummy_input,
            str(output_path),
            export_params=True,
            opset_version=14,
            do_constant_folding=True,
            input_names=['audio_input'],
            output_names=['emotion_output'],
            dynamic_axes={
                'audio_input': {0: 'batch_size', 2: 'n_frames'},
                'emotion_output': {0: 'batch_size'}
            }
        )
        print(f"âœ… ONNX æ¨¡å‹å¯¼å‡ºæˆåŠŸ")
    except Exception as e:
        print(f"âŒ ONNX å¯¼å‡ºå¤±è´¥: {e}")
        return False
    
    print()
    
    # 4. éªŒè¯ ONNX æ¨¡å‹
    print(f"ğŸ” éªŒè¯ ONNX æ¨¡å‹...")
    try:
        onnx_model = onnx.load(str(output_path))
        onnx.checker.check_model(onnx_model)
        print(f"âœ… ONNX æ¨¡å‹éªŒè¯é€šè¿‡")
    except Exception as e:
        print(f"âŒ ONNX æ¨¡å‹éªŒè¯å¤±è´¥: {e}")
        return False
    
    print()
    
    # 5. æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
    print(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯:")
    print(f"   æ–‡ä»¶å¤§å°: {output_path.stat().st_size / 1024 / 1024:.2f} MB")
    print(f"   è¾“å…¥å½¢çŠ¶: (batch, 80, n_frames)")
    print(f"   è¾“å‡ºå½¢çŠ¶: (batch, 7)")
    print(f"   å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    print()
    
    # 6. æµ‹è¯•æ¨ç†
    print(f"ğŸ§ª æµ‹è¯• ONNX æ¨ç†...")
    try:
        import onnxruntime as ort
        
        session = ort.InferenceSession(str(output_path))
        
        # å‡†å¤‡è¾“å…¥
        input_data = np.random.randn(1, 80, 100).astype(np.float32)
        
        # è¿è¡Œæ¨ç†
        outputs = session.run(None, {'audio_input': input_data})
        
        print(f"âœ… ONNX æ¨ç†æˆåŠŸ")
        print(f"   è¾“å‡ºå½¢çŠ¶: {outputs[0].shape}")
        print(f"   è¾“å‡ºå€¼: {outputs[0][0]}")
        
        # åº”ç”¨ softmax
        logits = outputs[0][0]
        probs = np.exp(logits) / np.sum(np.exp(logits))
        
        print()
        print(f"ğŸ“Š æƒ…æ„Ÿæ¦‚ç‡åˆ†å¸ƒ:")
        for i, (label, prob) in enumerate(zip(emotion_labels, probs)):
            print(f"   {label:12s}: {prob:.4f}")
        
        predicted_emotion = emotion_labels[np.argmax(probs)]
        print()
        print(f"ğŸ¯ é¢„æµ‹æƒ…æ„Ÿ: {predicted_emotion} (ç½®ä¿¡åº¦: {np.max(probs):.4f})")
        
    except Exception as e:
        print(f"âŒ ONNX æ¨ç†å¤±è´¥: {e}")
        return False
    
    print()
    print(f"=" * 80)
    print(f"ğŸ‰ Emotion Detection æ¨¡å‹åˆ›å»ºå®Œæˆï¼")
    print(f"=" * 80)
    print()
    print(f"ğŸ“ æ³¨æ„äº‹é¡¹:")
    print(f"   1. è¿™æ˜¯ä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„æ¨¡å‹")
    print(f"   2. å®é™…ä½¿ç”¨éœ€è¦åœ¨æƒ…æ„Ÿæ•°æ®é›†ä¸Šè®­ç»ƒ")
    print(f"   3. æˆ–è€…ä½¿ç”¨é¢„è®­ç»ƒçš„ Wav2Vec2 + æƒ…æ„Ÿåˆ†ç±»å¤´")
    print()
    
    return True


def download_pretrained_emotion_model(output_dir="/work/models"):
    """
    ä¸‹è½½é¢„è®­ç»ƒçš„æƒ…æ„Ÿæ£€æµ‹æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    
    æ³¨æ„: è¿™éœ€è¦ transformers åº“å’Œ HuggingFace æ¨¡å‹
    """
    print(f"=" * 80)
    print(f"ğŸ¯ ä¸‹è½½é¢„è®­ç»ƒçš„ Emotion Detection æ¨¡å‹")
    print(f"=" * 80)
    print()
    
    try:
        from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor
        
        model_name = "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition"
        
        print(f"ğŸ“¥ ä¸‹è½½æ¨¡å‹: {model_name}")
        print(f"   (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...)")
        
        processor = Wav2Vec2Processor.from_pretrained(model_name)
        model = Wav2Vec2ForSequenceClassification.from_pretrained(model_name)
        model.eval()
        
        print(f"âœ… æ¨¡å‹ä¸‹è½½æˆåŠŸ")
        print()
        
        # å¯¼å‡ºä¸º ONNX
        # æ³¨æ„: Wav2Vec2 æ¨¡å‹è¾ƒå¤§ï¼Œå¯¼å‡ºå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´
        print(f"ğŸ’¾ å¯¼å‡º ONNX æ¨¡å‹...")
        print(f"   âš ï¸ è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...")
        
        # åˆ›å»ºç¤ºä¾‹è¾“å…¥
        dummy_input = torch.randn(1, 16000)  # 1 ç§’éŸ³é¢‘ @ 16kHz
        
        output_path = Path(output_dir) / "emotion-model.onnx"
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        torch.onnx.export(
            model,
            dummy_input,
            str(output_path),
            export_params=True,
            opset_version=14,
            input_names=['audio_input'],
            output_names=['emotion_output'],
            dynamic_axes={
                'audio_input': {0: 'batch_size', 1: 'sequence_length'}
            }
        )
        
        print(f"âœ… ONNX æ¨¡å‹å¯¼å‡ºæˆåŠŸ")
        print(f"   æ–‡ä»¶å¤§å°: {output_path.stat().st_size / 1024 / 1024:.2f} MB")
        
        return True
        
    except ImportError:
        print(f"âŒ éœ€è¦å®‰è£… transformers åº“:")
        print(f"   pip install transformers")
        return False
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return False


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="åˆ›å»º Emotion Detection æ¨¡å‹")
    parser.add_argument(
        "--output-dir",
        type=str,
        default="/work/models",
        help="è¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        "--pretrained",
        action="store_true",
        help="ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹ï¼ˆéœ€è¦ transformers åº“ï¼‰"
    )
    
    args = parser.parse_args()
    
    if args.pretrained:
        success = download_pretrained_emotion_model(args.output_dir)
    else:
        success = create_emotion_model(args.output_dir)
    
    sys.exit(0 if success else 1)

