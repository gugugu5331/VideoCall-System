#!/usr/bin/env python3
"""
æ­£ç¡®å¯¼å‡º Whisper æ¨¡å‹ä¸º ONNX æ ¼å¼

è§£å†³ä½ç½®ç¼–ç ç»´åº¦ä¸åŒ¹é…çš„é—®é¢˜
"""

import torch
import onnx
import numpy as np
from pathlib import Path
import sys


def export_whisper_encoder_onnx():
    """å¯¼å‡º Whisper Encoder ä¸º ONNX"""
    print("=" * 80)
    print("ğŸ¯ å¯¼å‡º Whisper Encoder ä¸º ONNX")
    print("=" * 80)
    print()
    
    try:
        import whisper
        
        model_size = "base"
        
        print(f"ğŸ“¥ åŠ è½½ Whisper æ¨¡å‹: {model_size}")
        model = whisper.load_model(model_size, device="cpu")
        model = model.cpu()
        model.eval()
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        print()
        
        # è·å– Whisper çš„é…ç½®
        n_mels = 80
        n_audio_ctx = model.dims.n_audio_ctx  # 1500
        # Whisper ä½¿ç”¨ 2 ä¸ª conv1d å±‚ï¼Œæ¯ä¸ª stride=2
        # æ‰€ä»¥è¾“å…¥ mel-spectrogram é•¿åº¦éœ€è¦æ˜¯ n_audio_ctx * 2
        mel_length = n_audio_ctx * 2  # 3000

        print(f"ğŸ“Š Whisper é…ç½®:")
        print(f"   n_mels: {n_mels}")
        print(f"   n_audio_ctx: {n_audio_ctx}")
        print(f"   mel_length: {mel_length}")
        print(f"   n_audio_state: {model.dims.n_audio_state}")
        print()

        # å¯¼å‡º Encoderï¼ˆä½¿ç”¨å›ºå®šé•¿åº¦ï¼‰
        print(f"ğŸ’¾ å¯¼å‡º Encoder ä¸º ONNXï¼ˆå›ºå®šé•¿åº¦ï¼‰...")

        # åˆ›å»ºå›ºå®šé•¿åº¦çš„è¾“å…¥
        dummy_mel = torch.randn(1, n_mels, mel_length).cpu()
        
        encoder_output_path = Path("/work/models/whisper-encoder.onnx")
        
        with torch.no_grad():
            torch.onnx.export(
                model.encoder,
                dummy_mel,
                str(encoder_output_path),
                export_params=True,
                opset_version=14,
                do_constant_folding=True,
                input_names=['mel'],
                output_names=['encoder_output'],
                dynamic_axes={
                    'mel': {0: 'batch_size'},
                    'encoder_output': {0: 'batch_size'}
                }
            )
        
        print(f"âœ… Encoder ONNX å¯¼å‡ºæˆåŠŸ: {encoder_output_path}")
        print(f"   æ–‡ä»¶å¤§å°: {encoder_output_path.stat().st_size / 1024 / 1024:.2f} MB")
        print()
        
        # éªŒè¯æ¨¡å‹
        print(f"ğŸ” éªŒè¯ ONNX æ¨¡å‹...")
        onnx_model = onnx.load(str(encoder_output_path))
        onnx.checker.check_model(onnx_model)
        print(f"âœ… ONNX æ¨¡å‹éªŒè¯é€šè¿‡")
        print()
        
        # æµ‹è¯•æ¨ç†
        print(f"ğŸ§ª æµ‹è¯• ONNX æ¨ç†...")
        import onnxruntime as ort

        session = ort.InferenceSession(str(encoder_output_path))
        input_data = np.random.randn(1, n_mels, mel_length).astype(np.float32)
        outputs = session.run(None, {'mel': input_data})
        
        print(f"âœ… ONNX æ¨ç†æˆåŠŸ")
        print(f"   è¾“å…¥å½¢çŠ¶: {input_data.shape}")
        print(f"   è¾“å‡ºå½¢çŠ¶: {outputs[0].shape}")
        print()
        
        # ä¿å­˜ tokenizer å’Œé…ç½®
        print(f"ğŸ’¾ ä¿å­˜ Tokenizer å’Œé…ç½®...")
        
        tokenizer = whisper.tokenizer.get_tokenizer(multilingual=True)
        
        # ä¿å­˜å®Œæ•´çš„è¯æ±‡è¡¨
        vocab = {}
        for i in range(tokenizer.encoding.n_vocab):
            try:
                token = tokenizer.decode([i])
                vocab[i] = token
            except:
                vocab[i] = f"<token_{i}>"
        
        # ä¿å­˜ä¸º JSON
        import json
        
        vocab_path = Path("/work/models/whisper_vocab.json")
        with open(vocab_path, 'w', encoding='utf-8') as f:
            json.dump(vocab, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… è¯æ±‡è¡¨å·²ä¿å­˜: {vocab_path}")
        print(f"   è¯æ±‡è¡¨å¤§å°: {len(vocab)}")
        print()
        
        # ä¿å­˜ç‰¹æ®Š token
        special_tokens = {
            "sot": int(tokenizer.sot),
            "eot": int(tokenizer.eot),
            "sot_prev": int(tokenizer.sot_prev),
            "no_speech": int(tokenizer.no_speech),
            "no_timestamps": int(tokenizer.no_timestamps),
            "timestamp_begin": int(tokenizer.timestamp_begin),
            "language_tokens": {
                "zh": int(tokenizer.sot + 1 + tokenizer.encoding.encode(" Chinese")[0]),
                "en": int(tokenizer.sot + 1 + tokenizer.encoding.encode(" English")[0]),
            },
            "task_tokens": {
                "transcribe": int(tokenizer.transcribe),
                "translate": int(tokenizer.translate),
            }
        }
        
        special_tokens_path = Path("/work/models/whisper_special_tokens.json")
        with open(special_tokens_path, 'w', encoding='utf-8') as f:
            json.dump(special_tokens, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ç‰¹æ®Š token å·²ä¿å­˜: {special_tokens_path}")
        print()
        
        # ä¿å­˜æ¨¡å‹é…ç½®
        model_config = {
            "model_size": model_size,
            "n_mels": n_mels,
            "n_audio_ctx": n_audio_ctx,
            "mel_length": mel_length,
            "n_audio_state": model.dims.n_audio_state,
            "n_audio_head": model.dims.n_audio_head,
            "n_audio_layer": model.dims.n_audio_layer,
            "n_vocab": tokenizer.encoding.n_vocab,
            "n_text_ctx": model.dims.n_text_ctx,
            "n_text_state": model.dims.n_text_state,
            "n_text_head": model.dims.n_text_head,
            "n_text_layer": model.dims.n_text_layer,
        }
        
        config_path = Path("/work/models/whisper_config.json")
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(model_config, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… æ¨¡å‹é…ç½®å·²ä¿å­˜: {config_path}")
        print()
        
        print(f"=" * 80)
        print(f"ğŸ‰ Whisper Encoder ONNX å¯¼å‡ºå®Œæˆï¼")
        print(f"=" * 80)
        print()
        
        return True
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


def export_whisper_decoder_onnx():
    """å¯¼å‡º Whisper Decoder ä¸º ONNXï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
    print("=" * 80)
    print("ğŸ¯ åˆ›å»ºç®€åŒ–çš„ Whisper Decoder")
    print("=" * 80)
    print()
    
    try:
        import whisper
        
        model_size = "base"
        
        print(f"ğŸ“¥ åŠ è½½ Whisper æ¨¡å‹: {model_size}")
        model = whisper.load_model(model_size, device="cpu")
        model = model.cpu()
        model.eval()
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        print()
        
        # åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„ Decoder wrapper
        class WhisperDecoderWrapper(torch.nn.Module):
            def __init__(self, decoder):
                super().__init__()
                self.decoder = decoder
            
            def forward(self, tokens, encoder_output):
                # tokens: (batch, seq_len)
                # encoder_output: (batch, n_audio_ctx, n_audio_state)
                return self.decoder(tokens, encoder_output)
        
        decoder_wrapper = WhisperDecoderWrapper(model.decoder)
        decoder_wrapper.eval()
        
        print(f"ğŸ’¾ å¯¼å‡º Decoder ä¸º ONNX...")
        
        # åˆ›å»ºç¤ºä¾‹è¾“å…¥
        batch_size = 1
        seq_len = 10
        n_audio_ctx = 1500
        n_audio_state = model.dims.n_audio_state
        
        dummy_tokens = torch.tensor([[50258, 50259, 50359, 50363] + [0] * (seq_len - 4)], dtype=torch.long)
        dummy_encoder_output = torch.randn(batch_size, n_audio_ctx, n_audio_state)
        
        decoder_output_path = Path("/work/models/whisper-decoder.onnx")
        
        with torch.no_grad():
            torch.onnx.export(
                decoder_wrapper,
                (dummy_tokens, dummy_encoder_output),
                str(decoder_output_path),
                export_params=True,
                opset_version=14,
                do_constant_folding=True,
                input_names=['tokens', 'encoder_output'],
                output_names=['logits'],
                dynamic_axes={
                    'tokens': {0: 'batch_size', 1: 'seq_len'},
                    'encoder_output': {0: 'batch_size'},
                    'logits': {0: 'batch_size', 1: 'seq_len'}
                }
            )
        
        print(f"âœ… Decoder ONNX å¯¼å‡ºæˆåŠŸ: {decoder_output_path}")
        print(f"   æ–‡ä»¶å¤§å°: {decoder_output_path.stat().st_size / 1024 / 1024:.2f} MB")
        print()
        
        # éªŒè¯æ¨¡å‹
        print(f"ğŸ” éªŒè¯ ONNX æ¨¡å‹...")
        onnx_model = onnx.load(str(decoder_output_path))
        onnx.checker.check_model(onnx_model)
        print(f"âœ… ONNX æ¨¡å‹éªŒè¯é€šè¿‡")
        print()
        
        # æµ‹è¯•æ¨ç†
        print(f"ğŸ§ª æµ‹è¯• ONNX æ¨ç†...")
        import onnxruntime as ort
        
        session = ort.InferenceSession(str(decoder_output_path))
        
        tokens_data = dummy_tokens.numpy().astype(np.int64)
        encoder_output_data = dummy_encoder_output.numpy().astype(np.float32)
        
        outputs = session.run(None, {
            'tokens': tokens_data,
            'encoder_output': encoder_output_data
        })
        
        print(f"âœ… ONNX æ¨ç†æˆåŠŸ")
        print(f"   tokens å½¢çŠ¶: {tokens_data.shape}")
        print(f"   encoder_output å½¢çŠ¶: {encoder_output_data.shape}")
        print(f"   logits å½¢çŠ¶: {outputs[0].shape}")
        print()
        
        print(f"=" * 80)
        print(f"ğŸ‰ Whisper Decoder ONNX å¯¼å‡ºå®Œæˆï¼")
        print(f"=" * 80)
        print()
        
        return True
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="å¯¼å‡º Whisper æ¨¡å‹ä¸º ONNX")
    parser.add_argument(
        "--component",
        type=str,
        choices=["encoder", "decoder", "all"],
        default="all",
        help="å¯¼å‡ºå“ªä¸ªç»„ä»¶"
    )
    
    args = parser.parse_args()
    
    success = True
    
    if args.component in ["encoder", "all"]:
        if not export_whisper_encoder_onnx():
            success = False
    
    if args.component in ["decoder", "all"]:
        if not export_whisper_decoder_onnx():
            success = False
    
    sys.exit(0 if success else 1)

