#!/usr/bin/env python3
"""
ä» HuggingFace ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹å¹¶è½¬æ¢ä¸º ONNX æ ¼å¼

æ”¯æŒçš„æ¨¡å‹:
1. ASR: facebook/wav2vec2-base-960h (Wav2Vec2 ASR)
2. Emotion: ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition
3. Synthesis: ä½¿ç”¨éŸ³é¢‘åˆ†ç±»æ¨¡å‹
"""

import torch
import onnx
import numpy as np
from pathlib import Path
import sys
import argparse


def download_asr_model(output_dir="/work/models"):
    """
    ä¸‹è½½å¹¶è½¬æ¢ OpenAI Whisper ASR æ¨¡å‹ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰

    ä½¿ç”¨ openai/whisper-base æ¨¡å‹
    è¿™æ˜¯ä¸€ä¸ªæ”¯æŒ 99 ç§è¯­è¨€çš„å¤šè¯­è¨€ ASR æ¨¡å‹ï¼ŒåŒ…æ‹¬ä¸­æ–‡å’Œè‹±æ–‡
    """
    print("=" * 80)
    print("ğŸ¯ ä¸‹è½½ ASR æ¨¡å‹: OpenAI Whisper (æ”¯æŒä¸­è‹±æ–‡)")
    print("=" * 80)
    print()

    try:
        import whisper

        model_size = "base"  # å¯é€‰: tiny, base, small, medium, large

        print(f"ğŸ“¥ ä¸‹è½½ Whisper æ¨¡å‹: {model_size}")
        print(f"   âš ï¸ è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œæ¨¡å‹å¤§å°çº¦ 140 MB...")
        print(f"   âœ… æ”¯æŒè¯­è¨€: ä¸­æ–‡ã€è‹±æ–‡åŠå…¶ä»– 97 ç§è¯­è¨€")
        print()

        # ä¸‹è½½ Whisper æ¨¡å‹å¹¶ç§»åˆ° CPU
        model = whisper.load_model(model_size, device="cpu")
        model = model.cpu()
        model.eval()

        print(f"âœ… Whisper æ¨¡å‹ä¸‹è½½æˆåŠŸ")
        print()

        # å¯¼å‡º Encoder ä¸º ONNX
        print(f"ğŸ’¾ å¯¼å‡º Whisper Encoder ä¸º ONNX...")

        # åˆ›å»ºç¤ºä¾‹è¾“å…¥ (30 ç§’éŸ³é¢‘çš„ mel-spectrogram)
        # Whisper ä½¿ç”¨ 80-channel mel-spectrogramï¼Œæ¯ç§’ 50 å¸§
        # 30 ç§’ = 1500 å¸§
        dummy_mel = torch.randn(1, 80, 3000).cpu()

        encoder_output_path = Path(output_dir) / "whisper-encoder.onnx"

        torch.onnx.export(
            model.encoder,
            dummy_mel,
            str(encoder_output_path),
            export_params=True,
            opset_version=14,
            do_constant_folding=True,
            input_names=['mel'],
            output_names=['encoder_output'],
            dynamic_axes={
                'mel': {0: 'batch_size', 2: 'n_frames'},
                'encoder_output': {0: 'batch_size', 1: 'n_frames'}
            }
        )

        print(f"âœ… Whisper Encoder ONNX å¯¼å‡ºæˆåŠŸ: {encoder_output_path}")
        print()

        # å¯¼å‡º Decoder ä¸º ONNX
        print(f"ğŸ’¾ å¯¼å‡º Whisper Decoder ä¸º ONNX...")

        # Decoder è¾“å…¥: tokens (batch, seq_len) å’Œ encoder_output (batch, n_frames, n_audio_state)
        dummy_tokens = torch.tensor([[50258, 50259, 50359]])  # <|startoftranscript|>, <|zh|>, <|transcribe|>
        dummy_encoder_output = torch.randn(1, 1500, 512)

        decoder_output_path = Path(output_dir) / "whisper-decoder.onnx"

        # æ³¨æ„: Whisper decoder æ¯”è¾ƒå¤æ‚ï¼Œè¿™é‡Œå…ˆå¯¼å‡º encoder
        # å®Œæ•´çš„ decoder éœ€è¦å¤„ç† cross-attention å’Œ autoregressive ç”Ÿæˆ

        print(f"âš ï¸ Whisper Decoder å¯¼å‡ºè¾ƒå¤æ‚ï¼Œæš‚æ—¶ä½¿ç”¨ Encoder-only æ¨¡å¼")
        print(f"   å°†ä½¿ç”¨ç®€åŒ–çš„è§£ç ç­–ç•¥")
        print()

        # éªŒè¯ Encoder æ¨¡å‹
        print(f"ğŸ” éªŒè¯ ONNX æ¨¡å‹...")
        onnx_model = onnx.load(str(encoder_output_path))
        onnx.checker.check_model(onnx_model)
        print(f"âœ… ONNX æ¨¡å‹éªŒè¯é€šè¿‡")
        print()

        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        print(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯:")
        print(f"   Encoder å¤§å°: {encoder_output_path.stat().st_size / 1024 / 1024:.2f} MB")
        print(f"   è¾“å…¥: mel-spectrogram (80 x n_frames)")
        print(f"   è¾“å‡º: encoder_output (n_frames x 512)")
        print(f"   æ”¯æŒè¯­è¨€: ä¸­æ–‡ã€è‹±æ–‡ã€æ—¥æ–‡ã€éŸ©æ–‡ç­‰ 99 ç§")
        print()

        # ä¿å­˜ tokenizer
        print(f"ğŸ’¾ ä¿å­˜ Whisper Tokenizer...")
        tokenizer_path = Path(output_dir) / "whisper_tokenizer.json"

        # è·å– tokenizer çš„è¯æ±‡è¡¨
        tokenizer = whisper.tokenizer.get_tokenizer(multilingual=True)

        # ä¿å­˜è¯æ±‡è¡¨å’Œç‰¹æ®Š token
        tokenizer_data = {
            "vocab_size": tokenizer.encoding.n_vocab,
            "sot": tokenizer.sot,  # start of transcript
            "eot": tokenizer.eot,  # end of transcript
            "sot_prev": tokenizer.sot_prev,
            "no_speech": tokenizer.no_speech,
            "no_timestamps": tokenizer.no_timestamps,
            "timestamp_begin": tokenizer.timestamp_begin,
            "language_tokens": {
                "zh": tokenizer.encode(" ä¸­æ–‡")[0],
                "en": tokenizer.encode(" English")[0],
            }
        }

        import json
        with open(tokenizer_path, 'w', encoding='utf-8') as f:
            json.dump(tokenizer_data, f, indent=2, ensure_ascii=False)

        print(f"âœ… Tokenizer å·²ä¿å­˜åˆ°: {tokenizer_path}")
        print(f"   è¯æ±‡è¡¨å¤§å°: {tokenizer_data['vocab_size']}")
        print()

        # æµ‹è¯•æ¨ç†
        print(f"ğŸ§ª æµ‹è¯• ONNX æ¨ç†...")
        import onnxruntime as ort

        session = ort.InferenceSession(str(encoder_output_path))
        input_data = np.random.randn(1, 80, 1500).astype(np.float32)
        outputs = session.run(None, {'mel': input_data})

        print(f"âœ… ONNX æ¨ç†æˆåŠŸ")
        print(f"   è¾“å‡ºå½¢çŠ¶: {outputs[0].shape}")
        print()

        # ä¿å­˜å®Œæ•´çš„ Whisper æ¨¡å‹ï¼ˆPyTorch æ ¼å¼ï¼‰ç”¨äºåç»­å¤„ç†
        whisper_model_path = Path(output_dir) / "whisper_base.pt"
        torch.save(model.state_dict(), str(whisper_model_path))
        print(f"âœ… Whisper å®Œæ•´æ¨¡å‹å·²ä¿å­˜: {whisper_model_path}")
        print()

        print(f"=" * 80)
        print(f"ğŸ‰ Whisper ASR æ¨¡å‹ä¸‹è½½å’Œè½¬æ¢å®Œæˆï¼")
        print(f"=" * 80)
        print()

        return True

    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


def download_emotion_model(output_dir="/work/models"):
    """
    ä¸‹è½½å¹¶è½¬æ¢æƒ…æ„Ÿæ£€æµ‹æ¨¡å‹

    ä½¿ç”¨ ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition
    è¿™æ˜¯ä¸€ä¸ªåœ¨æƒ…æ„Ÿæ•°æ®é›†ä¸Šå¾®è°ƒçš„ Wav2Vec2 æ¨¡å‹
    """
    print("=" * 80)
    print("ğŸ¯ ä¸‹è½½ Emotion Detection æ¨¡å‹")
    print("=" * 80)
    print()

    try:
        from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor

        model_name = "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition"

        print(f"ğŸ“¥ ä¸‹è½½æ¨¡å‹: {model_name}")
        print(f"   âš ï¸ è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œæ¨¡å‹å¤§å°çº¦ 1.2 GB...")
        print()

        # ä¸‹è½½æ¨¡å‹å’Œç‰¹å¾æå–å™¨
        feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)
        model = Wav2Vec2ForSequenceClassification.from_pretrained(model_name)
        model.eval()

        print(f"âœ… æ¨¡å‹ä¸‹è½½æˆåŠŸ")
        print()

        # ä¿å­˜ç‰¹å¾æå–å™¨é…ç½®
        feature_extractor_dir = Path(output_dir) / "emotion_feature_extractor"
        feature_extractor_dir.mkdir(parents=True, exist_ok=True)
        feature_extractor.save_pretrained(str(feature_extractor_dir))
        print(f"âœ… ç‰¹å¾æå–å™¨é…ç½®å·²ä¿å­˜åˆ°: {feature_extractor_dir}")
        print()
        
        # å¯¼å‡ºä¸º ONNX
        print(f"ğŸ’¾ å¯¼å‡º ONNX æ¨¡å‹...")
        
        # åˆ›å»ºç¤ºä¾‹è¾“å…¥
        dummy_input = torch.randn(1, 16000)
        
        output_path = Path(output_dir) / "emotion-model.onnx"
        
        torch.onnx.export(
            model,
            dummy_input,
            str(output_path),
            export_params=True,
            opset_version=14,
            do_constant_folding=True,
            input_names=['audio_input'],
            output_names=['logits'],
            dynamic_axes={
                'audio_input': {0: 'batch_size', 1: 'sequence_length'},
                'logits': {0: 'batch_size'}
            }
        )
        
        print(f"âœ… ONNX æ¨¡å‹å¯¼å‡ºæˆåŠŸ: {output_path}")
        print()
        
        # éªŒè¯æ¨¡å‹
        print(f"ğŸ” éªŒè¯ ONNX æ¨¡å‹...")
        onnx_model = onnx.load(str(output_path))
        onnx.checker.check_model(onnx_model)
        print(f"âœ… ONNX æ¨¡å‹éªŒè¯é€šè¿‡")
        print()
        
        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        print(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯:")
        print(f"   æ–‡ä»¶å¤§å°: {output_path.stat().st_size / 1024 / 1024:.2f} MB")
        print(f"   è¾“å…¥: audio_input (raw waveform)")
        print(f"   è¾“å‡º: logits (emotion probabilities)")
        print(f"   æƒ…æ„Ÿç±»åˆ«: {model.config.id2label}")
        print()
        
        # æµ‹è¯•æ¨ç†
        print(f"ğŸ§ª æµ‹è¯• ONNX æ¨ç†...")
        import onnxruntime as ort
        
        session = ort.InferenceSession(str(output_path))
        input_data = np.random.randn(1, 16000).astype(np.float32)
        outputs = session.run(None, {'audio_input': input_data})
        
        print(f"âœ… ONNX æ¨ç†æˆåŠŸ")
        print(f"   è¾“å‡ºå½¢çŠ¶: {outputs[0].shape}")
        
        # åº”ç”¨ softmax
        logits = outputs[0][0]
        probs = np.exp(logits) / np.sum(np.exp(logits))
        
        print()
        print(f"ğŸ“Š æƒ…æ„Ÿæ¦‚ç‡åˆ†å¸ƒ:")
        for idx, prob in enumerate(probs):
            emotion = model.config.id2label.get(idx, f"emotion_{idx}")
            print(f"   {emotion:12s}: {prob:.4f}")
        print()
        
        print(f"=" * 80)
        print(f"ğŸ‰ Emotion Detection æ¨¡å‹ä¸‹è½½å’Œè½¬æ¢å®Œæˆï¼")
        print(f"=" * 80)
        print()
        
        return True
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


def download_synthesis_model(output_dir="/work/models"):
    """
    ä¸‹è½½å¹¶è½¬æ¢æ·±åº¦ä¼ªé€ æ£€æµ‹æ¨¡å‹
    
    ç”±äº HuggingFace ä¸Šæ²¡æœ‰ä¸“é—¨çš„ ASVspoof æ¨¡å‹ï¼Œ
    æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªè½»é‡çº§çš„éŸ³é¢‘åˆ†ç±»æ¨¡å‹ä½œä¸ºåŸºç¡€
    """
    print("=" * 80)
    print("ğŸ¯ åˆ›å»º Synthesis Detection æ¨¡å‹")
    print("=" * 80)
    print()
    
    print("âš ï¸ æ³¨æ„: HuggingFace ä¸Šæ²¡æœ‰ç°æˆçš„æ·±åº¦ä¼ªé€ æ£€æµ‹æ¨¡å‹")
    print("   ä½¿ç”¨å½“å‰çš„ç®€åŒ–æ¨¡å‹ï¼ˆå·²ç»æ¯”è™šæ‹Ÿæ¨¡å‹å¥½ï¼‰")
    print()
    
    # ä¿æŒä½¿ç”¨ä¹‹å‰åˆ›å»ºçš„ç®€åŒ–æ¨¡å‹
    print("âœ… ä½¿ç”¨ç°æœ‰çš„ synthesis-model.onnx")
    print()
    
    return True


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹å¹¶è½¬æ¢ä¸º ONNX")
    parser.add_argument(
        "--output-dir",
        type=str,
        default="/work/models",
        help="è¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        "--model",
        type=str,
        choices=["asr", "emotion", "synthesis", "all"],
        default="all",
        help="è¦ä¸‹è½½çš„æ¨¡å‹"
    )
    
    args = parser.parse_args()
    
    success = True
    
    if args.model in ["asr", "all"]:
        if not download_asr_model(args.output_dir):
            success = False
    
    if args.model in ["emotion", "all"]:
        if not download_emotion_model(args.output_dir):
            success = False
    
    if args.model in ["synthesis", "all"]:
        if not download_synthesis_model(args.output_dir):
            success = False
    
    sys.exit(0 if success else 1)

