#!/usr/bin/env python3
"""
å®Œæ•´å¯¼å‡º Whisper æ¨¡å‹ä¸º ONNXï¼ˆåŒ…æ‹¬ Encoder å’Œ Decoderï¼‰

è§£å†³ scaled_dot_product_attention çš„ is_causal å‚æ•°é—®é¢˜
é€šè¿‡ monkey-patch ä¿®æ”¹ Whisper çš„ attention å®ç°
"""

import torch
import torch.nn.functional as F
import onnx
import numpy as np
from pathlib import Path
import sys


def patch_whisper_for_onnx():
    """ä¿®æ”¹ Whisper æ¨¡å‹ä»¥å…¼å®¹ ONNX å¯¼å‡º"""
    import whisper.model as whisper_model
    
    # ä¿å­˜åŸå§‹çš„ qkv_attention æ–¹æ³•
    original_qkv_attention = whisper_model.MultiHeadAttention.qkv_attention
    
    def onnx_compatible_qkv_attention(self, q, k, v, mask=None):
        """ONNX å…¼å®¹çš„ QKV attention"""
        n_batch, n_ctx, n_state = q.shape
        scale = (n_state // self.n_head) ** -0.25
        q = q.view(*q.shape[:2], self.n_head, -1).permute(0, 2, 1, 3) * scale
        k = k.view(*k.shape[:2], self.n_head, -1).permute(0, 2, 3, 1) * scale
        v = v.view(*v.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)

        # ä½¿ç”¨ä¼ ç»Ÿçš„ attention è®¡ç®—ï¼Œé¿å… scaled_dot_product_attention
        qk = q @ k
        if mask is not None:
            qk = qk + mask[:n_ctx, :n_ctx]
        qk = qk.float()

        w = F.softmax(qk, dim=-1).to(q.dtype)
        out = (w @ v).permute(0, 2, 1, 3).flatten(start_dim=2)
        return out, qk.detach()
    
    # æ›¿æ¢æ–¹æ³•
    whisper_model.MultiHeadAttention.qkv_attention = onnx_compatible_qkv_attention
    
    print("âœ… Whisper æ¨¡å‹å·²ä¿®æ”¹ä¸º ONNX å…¼å®¹æ¨¡å¼")
    
    return original_qkv_attention


def export_whisper_encoder():
    """å¯¼å‡º Whisper Encoder"""
    print("=" * 80)
    print("ğŸ¯ å¯¼å‡º Whisper Encoder ä¸º ONNX")
    print("=" * 80)
    print()
    
    try:
        import whisper
        
        model_size = "base"
        
        print(f"ğŸ“¥ åŠ è½½ Whisper æ¨¡å‹: {model_size}")
        model = whisper.load_model(model_size, device="cpu")
        model = model.cpu()
        model.eval()
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        print()
        
        # è·å–é…ç½®
        n_mels = 80
        n_audio_ctx = model.dims.n_audio_ctx
        mel_length = n_audio_ctx * 2
        
        print(f"ğŸ“Š Encoder é…ç½®:")
        print(f"   n_mels: {n_mels}")
        print(f"   n_audio_ctx: {n_audio_ctx}")
        print(f"   mel_length: {mel_length}")
        print()
        
        # å¯¼å‡º Encoder
        print(f"ğŸ’¾ å¯¼å‡º Encoder ä¸º ONNX...")
        
        dummy_mel = torch.randn(1, n_mels, mel_length).cpu()
        encoder_output_path = Path("/work/models/whisper-encoder.onnx")
        
        with torch.no_grad():
            torch.onnx.export(
                model.encoder,
                dummy_mel,
                str(encoder_output_path),
                export_params=True,
                opset_version=14,
                do_constant_folding=True,
                input_names=['mel'],
                output_names=['encoder_output'],
                dynamic_axes={
                    'mel': {0: 'batch_size'},
                    'encoder_output': {0: 'batch_size'}
                }
            )
        
        print(f"âœ… Encoder ONNX å¯¼å‡ºæˆåŠŸ: {encoder_output_path}")
        print(f"   æ–‡ä»¶å¤§å°: {encoder_output_path.stat().st_size / 1024 / 1024:.2f} MB")
        print()
        
        return True
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


def export_whisper_decoder():
    """å¯¼å‡º Whisper Decoderï¼ˆå®Œæ•´ç‰ˆæœ¬ï¼‰"""
    print("=" * 80)
    print("ğŸ¯ å¯¼å‡º Whisper Decoder ä¸º ONNXï¼ˆå®Œæ•´ç‰ˆæœ¬ï¼‰")
    print("=" * 80)
    print()
    
    try:
        import whisper
        
        # Patch Whisper for ONNX compatibility
        original_qkv_attention = patch_whisper_for_onnx()
        
        model_size = "base"
        
        print(f"ğŸ“¥ åŠ è½½ Whisper æ¨¡å‹: {model_size}")
        model = whisper.load_model(model_size, device="cpu")
        model = model.cpu()
        model.eval()
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        print()
        
        # è·å–é…ç½®
        n_audio_ctx = model.dims.n_audio_ctx
        n_audio_state = model.dims.n_audio_state
        n_text_ctx = model.dims.n_text_ctx
        
        print(f"ğŸ“Š Decoder é…ç½®:")
        print(f"   n_audio_ctx: {n_audio_ctx}")
        print(f"   n_audio_state: {n_audio_state}")
        print(f"   n_text_ctx: {n_text_ctx}")
        print()
        
        # åˆ›å»º Decoder wrapperï¼ˆå•æ­¥è§£ç ï¼‰
        class WhisperDecoderOneStep(torch.nn.Module):
            def __init__(self, decoder, n_text_ctx):
                super().__init__()
                self.decoder = decoder
                self.n_text_ctx = n_text_ctx
                
                # åˆ›å»ºå› æœæ©ç 
                self.register_buffer(
                    "mask",
                    torch.empty(n_text_ctx, n_text_ctx).fill_(-np.inf).triu_(1)
                )
            
            def forward(self, tokens, encoder_output):
                """
                å•æ­¥è§£ç 
                tokens: (batch, seq_len) - å½“å‰å·²ç”Ÿæˆçš„ token åºåˆ—
                encoder_output: (batch, n_audio_ctx, n_audio_state) - encoder è¾“å‡º
                è¿”å›: (batch, seq_len, n_vocab) - æ¯ä¸ªä½ç½®çš„ logits
                """
                # ä½¿ç”¨ decoder çš„ forward æ–¹æ³•
                # æ³¨æ„ï¼šéœ€è¦ä¼ é€’æ­£ç¡®çš„ mask
                x = self.decoder(tokens, encoder_output)
                return x
        
        decoder_wrapper = WhisperDecoderOneStep(model.decoder, n_text_ctx)
        decoder_wrapper.eval()
        
        print(f"ğŸ’¾ å¯¼å‡º Decoder ä¸º ONNX...")
        
        # åˆ›å»ºç¤ºä¾‹è¾“å…¥
        batch_size = 1
        seq_len = 10  # å½“å‰åºåˆ—é•¿åº¦
        
        # ç¤ºä¾‹ tokens: [<|startoftranscript|>, <|zh|>, <|transcribe|>, <|notimestamps|>, ...]
        dummy_tokens = torch.tensor([[50258, 50260, 50359, 50363] + [0] * (seq_len - 4)], dtype=torch.long)
        dummy_encoder_output = torch.randn(batch_size, n_audio_ctx, n_audio_state)
        
        decoder_output_path = Path("/work/models/whisper-decoder.onnx")
        
        with torch.no_grad():
            torch.onnx.export(
                decoder_wrapper,
                (dummy_tokens, dummy_encoder_output),
                str(decoder_output_path),
                export_params=True,
                opset_version=14,
                do_constant_folding=True,
                input_names=['tokens', 'encoder_output'],
                output_names=['logits'],
                dynamic_axes={
                    'tokens': {0: 'batch_size', 1: 'seq_len'},
                    'encoder_output': {0: 'batch_size'},
                    'logits': {0: 'batch_size', 1: 'seq_len'}
                }
            )
        
        print(f"âœ… Decoder ONNX å¯¼å‡ºæˆåŠŸ: {decoder_output_path}")
        print(f"   æ–‡ä»¶å¤§å°: {decoder_output_path.stat().st_size / 1024 / 1024:.2f} MB")
        print()
        
        # éªŒè¯æ¨¡å‹
        print(f"ğŸ” éªŒè¯ ONNX æ¨¡å‹...")
        onnx_model = onnx.load(str(decoder_output_path))
        onnx.checker.check_model(onnx_model)
        print(f"âœ… ONNX æ¨¡å‹éªŒè¯é€šè¿‡")
        print()
        
        # æµ‹è¯•æ¨ç†
        print(f"ğŸ§ª æµ‹è¯• ONNX æ¨ç†...")
        import onnxruntime as ort
        
        session = ort.InferenceSession(str(decoder_output_path))
        
        tokens_data = dummy_tokens.numpy().astype(np.int64)
        encoder_output_data = dummy_encoder_output.numpy().astype(np.float32)
        
        outputs = session.run(None, {
            'tokens': tokens_data,
            'encoder_output': encoder_output_data
        })
        
        print(f"âœ… ONNX æ¨ç†æˆåŠŸ")
        print(f"   tokens å½¢çŠ¶: {tokens_data.shape}")
        print(f"   encoder_output å½¢çŠ¶: {encoder_output_data.shape}")
        print(f"   logits å½¢çŠ¶: {outputs[0].shape}")
        print()
        
        # æµ‹è¯•è´ªå©ªè§£ç 
        print(f"ğŸ§ª æµ‹è¯•è´ªå©ªè§£ç ...")
        logits = outputs[0]  # (batch, seq_len, n_vocab)
        
        # è·å–æœ€åä¸€ä¸ª token çš„é¢„æµ‹
        last_logits = logits[0, -1, :]  # (n_vocab,)
        predicted_token = np.argmax(last_logits)
        
        print(f"   æœ€åä¸€ä¸ªä½ç½®çš„é¢„æµ‹ token: {predicted_token}")
        print(f"   æ¦‚ç‡: {np.exp(last_logits[predicted_token]) / np.sum(np.exp(last_logits)):.4f}")
        print()
        
        print(f"=" * 80)
        print(f"ğŸ‰ Whisper Decoder ONNX å¯¼å‡ºå®Œæˆï¼")
        print(f"=" * 80)
        print()
        
        return True
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


def save_model_info():
    """ä¿å­˜æ¨¡å‹ä¿¡æ¯å’Œé…ç½®"""
    print("=" * 80)
    print("ğŸ¯ ä¿å­˜æ¨¡å‹ä¿¡æ¯å’Œé…ç½®")
    print("=" * 80)
    print()
    
    try:
        import whisper
        import json
        
        model_size = "base"
        model = whisper.load_model(model_size, device="cpu")
        tokenizer = whisper.tokenizer.get_tokenizer(multilingual=True)
        
        # ä¿å­˜å®Œæ•´çš„è¯æ±‡è¡¨
        print("ğŸ’¾ ä¿å­˜è¯æ±‡è¡¨...")
        vocab = {}
        for i in range(tokenizer.encoding.n_vocab):
            try:
                token = tokenizer.decode([i])
                vocab[i] = token
            except:
                vocab[i] = f"<token_{i}>"
        
        vocab_path = Path("/work/models/whisper_vocab.json")
        with open(vocab_path, 'w', encoding='utf-8') as f:
            json.dump(vocab, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… è¯æ±‡è¡¨å·²ä¿å­˜: {vocab_path}")
        print(f"   è¯æ±‡è¡¨å¤§å°: {len(vocab)}")
        print()
        
        # ä¿å­˜ç‰¹æ®Š token
        print("ğŸ’¾ ä¿å­˜ç‰¹æ®Š token...")
        special_tokens = {
            "sot": int(tokenizer.sot),
            "eot": int(tokenizer.eot),
            "sot_prev": int(tokenizer.sot_prev),
            "no_speech": int(tokenizer.no_speech),
            "no_timestamps": int(tokenizer.no_timestamps),
            "timestamp_begin": int(tokenizer.timestamp_begin),
            "language_tokens": {
                "zh": int(tokenizer.sot + 1 + 50259 - tokenizer.sot),  # <|zh|>
                "en": int(tokenizer.sot + 1 + 50258 - tokenizer.sot),  # <|en|>
            },
            "task_tokens": {
                "transcribe": int(tokenizer.transcribe),
                "translate": int(tokenizer.translate),
            }
        }
        
        special_tokens_path = Path("/work/models/whisper_special_tokens.json")
        with open(special_tokens_path, 'w', encoding='utf-8') as f:
            json.dump(special_tokens, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ç‰¹æ®Š token å·²ä¿å­˜: {special_tokens_path}")
        print()
        
        # ä¿å­˜æ¨¡å‹é…ç½®
        print("ğŸ’¾ ä¿å­˜æ¨¡å‹é…ç½®...")
        model_config = {
            "model_size": model_size,
            "n_mels": 80,
            "n_audio_ctx": model.dims.n_audio_ctx,
            "mel_length": model.dims.n_audio_ctx * 2,
            "n_audio_state": model.dims.n_audio_state,
            "n_audio_head": model.dims.n_audio_head,
            "n_audio_layer": model.dims.n_audio_layer,
            "n_vocab": tokenizer.encoding.n_vocab,
            "n_text_ctx": model.dims.n_text_ctx,
            "n_text_state": model.dims.n_text_state,
            "n_text_head": model.dims.n_text_head,
            "n_text_layer": model.dims.n_text_layer,
        }
        
        config_path = Path("/work/models/whisper_config.json")
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(model_config, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… æ¨¡å‹é…ç½®å·²ä¿å­˜: {config_path}")
        print()
        
        print(f"=" * 80)
        print(f"ğŸ‰ æ¨¡å‹ä¿¡æ¯ä¿å­˜å®Œæˆï¼")
        print(f"=" * 80)
        print()
        
        return True
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="å®Œæ•´å¯¼å‡º Whisper æ¨¡å‹ä¸º ONNX")
    parser.add_argument(
        "--component",
        type=str,
        choices=["encoder", "decoder", "all"],
        default="all",
        help="å¯¼å‡ºå“ªä¸ªç»„ä»¶"
    )
    
    args = parser.parse_args()
    
    success = True
    
    if args.component in ["encoder", "all"]:
        if not export_whisper_encoder():
            success = False
    
    if args.component in ["decoder", "all"]:
        if not export_whisper_decoder():
            success = False
    
    if args.component == "all":
        if not save_model_info():
            success = False
    
    sys.exit(0 if success else 1)

