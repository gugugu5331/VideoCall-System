# Example: route /api/v1/ai/* to external GPU AI nodes
#
# Usage:
# - Copy to a real conf file (e.g. ai_inference_pool.conf)
# - Replace hosts/ports with your reachable endpoints
# - Ensure your nginx.conf includes this file (include /etc/nginx/conf.d/*.conf;)
#
# NOTE: Do not commit real infrastructure addresses into the repo.

upstream ai_inference_pool {
  least_conn;
  server <gpu-node-1-host>:<public-port> max_fails=3 fail_timeout=30s;
  server <gpu-node-2-host>:<public-port> max_fails=3 fail_timeout=30s;
  keepalive 16;
}
