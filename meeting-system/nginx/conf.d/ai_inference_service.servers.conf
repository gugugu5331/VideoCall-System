# Default upstream servers for ai_inference_service.
#
# Replace these lines in your deployment (or mount a different file) to route
# /api/v1/ai/* to external GPU AI nodes.
#
# NOTE: Do not commit real infrastructure addresses into the repo.

# A dummy primary server is required so nginx accepts configurations where the
# only real fallback is marked as `backup`.
server 127.0.0.1:1 down;

# Local (Docker) fallback (ai-inference-service -> Triton inside compose). Marked
# as `backup` so external GPU nodes (if provided via
# `ai_inference_service.servers.local.conf`) are preferred.
server ai-inference-service:8085 max_fails=3 fail_timeout=30s backup;
