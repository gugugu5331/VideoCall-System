version: "3.9"

services:
  triton:
    image: nvcr.io/nvidia/tritonserver:23.10-py3
    container_name: meeting-triton
    restart: unless-stopped
    command:
      - tritonserver
      - --model-repository=/models
      - --log-verbose=0
    ports:
      - "${TRITON_HTTP_PORT:-8000}:8000"
      - "${TRITON_GRPC_PORT:-8001}:8001"
      - "${TRITON_METRICS_PORT:-8002}:8002"
    volumes:
      - ${MODEL_DIR:-/models}:/models:ro
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  ai-inference-service:
    build:
      context: ../../backend
      dockerfile: ai-inference-service/Dockerfile.gpu
    image: meeting-ai-inference-service:gpu
    container_name: meeting-ai-inference-service
    restart: unless-stopped
    environment:
      - CONFIG_PATH=/app/config/ai-inference-service.yaml
    ports:
      - "${AI_HTTP_PORT:-8800}:8085"
      - "${AI_GRPC_PORT:-9800}:9085"
    volumes:
      - ${MODEL_DIR:-/models}:/models:ro
      - ../../backend/ai-inference-service/config:/app/config:ro
    depends_on:
      - triton
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
