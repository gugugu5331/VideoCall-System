# AIæ¨¡å‹éƒ¨ç½²æŒ‡å—

> æ³¨æ„ï¼šæœ¬æŒ‡å—é’ˆå¯¹å†å²çš„ Python/ONNX æ¨ç†é“¾è·¯ï¼Œå·²ä¸å†ä½œä¸ºé»˜è®¤æ–¹æ¡ˆã€‚å½“å‰ Triton/TensorRT éƒ¨ç½²è¯·å‚è€ƒ `docs/DEPLOYMENT/GPU_AI_NODES.md`ã€‚

## âœ… å·²å®Œæˆçš„å·¥ä½œ

### 1. åˆ é™¤æ‰€æœ‰æ¨¡æ‹Ÿ/é™çº§é€»è¾‘
- âœ… ä» `ai_manager.go` ä¸­åˆ é™¤ `getFallbackResponse()` æ–¹æ³•
- âœ… æ¢å¤ä¸¥æ ¼çš„çœŸå®æ¨ç†è¦æ±‚
- âœ… ç¡®ä¿æ‰€æœ‰æ¨ç†è¯·æ±‚å¿…é¡»ä½¿ç”¨çœŸå®æ¨¡å‹

### 2. åˆ›å»ºPythonæ¨ç†æœåŠ¡
- âœ… åˆ›å»º `Dockerfile.inference` - Pythonæ¨ç†æœåŠ¡å®¹å™¨
- âœ… åˆ›å»º `inference_server.py` - çœŸå®æ¨¡å‹æ¨ç†è„šæœ¬
- âœ… åˆ›å»º `requirements.txt` - Pythonä¾èµ–åˆ—è¡¨
- âœ… åˆ›å»º `download_all_models.sh` - è‡ªåŠ¨ä¸‹è½½æ‰€æœ‰7ä¸ªæ¨¡å‹

### 3. æ›´æ–°AIæœåŠ¡
- âœ… ä¿®æ”¹ `real_inference_service.go` è°ƒç”¨Pythonæ¨ç†
- âœ… å®ç° `callPythonInference()` æ–¹æ³•
- âœ… æ›´æ–°æ‰€æœ‰æ¨ç†æ–¹æ³•ï¼ˆSpeechRecognition, EmotionDetection, Summarizeç­‰ï¼‰

### 4. æ›´æ–°Dockeré…ç½®
- âœ… åœ¨ `docker-compose.yml` ä¸­æ·»åŠ  `python-inference` æœåŠ¡
- âœ… é…ç½®GPUæ”¯æŒï¼ˆNVIDIAï¼‰
- âœ… é…ç½®æ¨¡å‹å·æŒ‚è½½

## ğŸ“‹ éœ€è¦æ‰‹åŠ¨æ‰§è¡Œçš„æ­¥éª¤

### æ­¥éª¤1ï¼šæ„å»ºPythonæ¨ç†æœåŠ¡

```bash
cd meeting-system

# æ„å»ºPythonæ¨ç†æœåŠ¡é•œåƒï¼ˆè¿™å°†è‡ªåŠ¨ä¸‹è½½æ‰€æœ‰æ¨¡å‹ï¼‰
docker compose build python-inference

# æ³¨æ„ï¼šè¿™ä¸ªè¿‡ç¨‹å¯èƒ½éœ€è¦30-60åˆ†é’Ÿï¼Œå› ä¸ºéœ€è¦ä¸‹è½½çº¦1.3GBçš„æ¨¡å‹æ–‡ä»¶
```

### æ­¥éª¤2ï¼šå¯åŠ¨Pythonæ¨ç†æœåŠ¡

```bash
# å¯åŠ¨Pythonæ¨ç†æœåŠ¡
docker compose up -d python-inference

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
docker compose ps python-inference

# æŸ¥çœ‹æ—¥å¿—
docker compose logs -f python-inference
```

### æ­¥éª¤3ï¼šéªŒè¯æ¨¡å‹ä¸‹è½½

```bash
# è¿›å…¥å®¹å™¨æ£€æŸ¥æ¨¡å‹
docker exec -it meeting-python-inference bash

# åœ¨å®¹å™¨å†…æ‰§è¡Œ
ls -lah /models/
du -sh /models/*

# åº”è¯¥çœ‹åˆ°7ä¸ªæ¨¡å‹ç›®å½•ï¼Œæ¯ä¸ªéƒ½æœ‰æ¨¡å‹æ–‡ä»¶
# speech_recognition/
# emotion_detection/
# text_summarization/
# audio_denoising/
# video_enhancement/
# audio_deepfake/
# face_deepfake/
```

### æ­¥éª¤4ï¼šé‡æ–°æ„å»ºå¹¶å¯åŠ¨AIæœåŠ¡

```bash
# é‡æ–°æ„å»ºAIæœåŠ¡
docker compose build ai-service

# é‡å¯AIæœåŠ¡
docker compose up -d ai-service

# æ£€æŸ¥AIæœåŠ¡æ—¥å¿—
docker compose logs -f ai-service
```

### æ­¥éª¤5ï¼šè¿è¡ŒE2Eæµ‹è¯•

```bash
cd meeting-system/backend/tests

# è¿è¡Œå®Œæ•´çš„E2Eæµ‹è¯•
go test -v -run TestE2EIntegration

# é¢„æœŸç»“æœï¼šæ‰€æœ‰AIæ¨¡å‹æµ‹è¯•åº”è¯¥é€šè¿‡ï¼ŒæˆåŠŸç‡100%
```

## ğŸ”§ æ•…éšœæ’é™¤

### é—®é¢˜1ï¼šæ¨¡å‹ä¸‹è½½å¤±è´¥

å¦‚æœåœ¨Dockeræ„å»ºæ—¶æ¨¡å‹ä¸‹è½½å¤±è´¥ï¼Œå¯ä»¥æ‰‹åŠ¨ä¸‹è½½ï¼š

```bash
# è¿›å…¥è¿è¡Œä¸­çš„å®¹å™¨
docker exec -it meeting-python-inference bash

# æ‰‹åŠ¨è¿è¡Œä¸‹è½½è„šæœ¬
/app/download_models.sh

# æˆ–è€…é€ä¸ªä¸‹è½½
python3 << EOF
from huggingface_hub import snapshot_download
snapshot_download(
    repo_id="openai/whisper-tiny",
    local_dir="/models/speech_recognition",
    local_dir_use_symlinks=False
)
EOF
```

### é—®é¢˜2ï¼šGPUä¸å¯ç”¨

å¦‚æœæ²¡æœ‰GPUæˆ–GPUé©±åŠ¨é—®é¢˜ï¼š

```yaml
# ç¼–è¾‘ docker-compose.ymlï¼Œæ³¨é‡Šæ‰GPUé…ç½®
python-inference:
  # deploy:
  #   resources:
  #     reservations:
  #       devices:
  #         - driver: nvidia
  #           count: 1
  #           capabilities: [gpu]
```

ç„¶åé‡æ–°æ„å»ºå’Œå¯åŠ¨ã€‚

### é—®é¢˜3ï¼šå†…å­˜ä¸è¶³

å¦‚æœç³»ç»Ÿå†…å­˜ä¸è¶³ï¼Œå¯ä»¥ï¼š

1. å‡å°‘åŒæ—¶åŠ è½½çš„æ¨¡å‹æ•°é‡
2. ä½¿ç”¨æ›´å°çš„æ¨¡å‹
3. å¢åŠ swapç©ºé—´

```bash
# å¢åŠ swapç©ºé—´
sudo fallocate -l 8G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
```

### é—®é¢˜4ï¼šPythonæ¨ç†è°ƒç”¨å¤±è´¥

æ£€æŸ¥AIæœåŠ¡æ˜¯å¦èƒ½è®¿é—®Pythonæ¨ç†å®¹å™¨ï¼š

```bash
# ä»AIæœåŠ¡å®¹å™¨æµ‹è¯•
docker exec meeting-ai-service sh -c "docker exec meeting-python-inference echo 'test'"

# å¦‚æœå¤±è´¥ï¼Œå¯èƒ½éœ€è¦ä½¿ç”¨ç½‘ç»œè°ƒç”¨è€Œä¸æ˜¯docker exec
```

## ğŸ“Š æ¨¡å‹åˆ—è¡¨

| # | æ¨¡å‹ç±»å‹ | æ¨¡å‹ID | å¤§å° | ç”¨é€” |
|---|---------|--------|------|------|
| 1 | è¯­éŸ³è¯†åˆ« | openai/whisper-tiny | 39MB | ä¼šè®®è¯­éŸ³è½¬æ–‡å­— |
| 2 | æƒ…ç»ªæ£€æµ‹ | j-hartmann/emotion-english-distilroberta-base | 82MB | æ£€æµ‹è¯´è¯è€…æƒ…ç»ª |
| 3 | æ–‡æœ¬æ‘˜è¦ | sshleifer/distilbart-cnn-6-6 | 306MB | ä¼šè®®è®°å½•æ‘˜è¦ |
| 4 | éŸ³é¢‘é™å™ª | speechbrain/sepformer-wham | ~100MB | å®æ—¶éŸ³é¢‘é™å™ª |
| 5 | è§†é¢‘å¢å¼º | caidas/swin2SR-classical-sr-x2-64 | ~50MB | è§†é¢‘è´¨é‡æå‡ |
| 6 | éŸ³é¢‘ä¼ªé€ æ£€æµ‹ | microsoft/wavlm-base-plus | 378MB | æ£€æµ‹AIç”ŸæˆéŸ³é¢‘ |
| 7 | äººè„¸ä¼ªé€ æ£€æµ‹ | google/vit-base-patch16-224 | 346MB | æ£€æµ‹AIç”Ÿæˆäººè„¸ |

**æ€»å¤§å°**: ~1.3GB

## ğŸš€ æ¶æ„è¯´æ˜

### å½“å‰æ¶æ„

```
E2E Test
    â†“
AI Service (Go)
    â†“
docker exec â†’ Python Inference Container
                    â†“
              Transformers + PyTorch
                    â†“
              çœŸå®AIæ¨¡å‹æ¨ç†
```

### æ¨ç†æµç¨‹

1. E2Eæµ‹è¯•å‘é€è¯·æ±‚åˆ°AIæœåŠ¡ï¼ˆé€šè¿‡Nginxç½‘å…³ï¼‰
2. AIæœåŠ¡æ¥æ”¶è¯·æ±‚ï¼Œè°ƒç”¨ `RealInferenceService`
3. `RealInferenceService` é€šè¿‡ `docker exec` è°ƒç”¨Pythonæ¨ç†å®¹å™¨
4. Pythonå®¹å™¨åŠ è½½å¯¹åº”çš„æ¨¡å‹å¹¶æ‰§è¡Œæ¨ç†
5. æ¨ç†ç»“æœè¿”å›ç»™AIæœåŠ¡
6. AIæœåŠ¡è¿”å›ç»“æœç»™E2Eæµ‹è¯•

## âœ… éªŒè¯æ¸…å•

å®Œæˆéƒ¨ç½²åï¼ŒéªŒè¯ä»¥ä¸‹å†…å®¹ï¼š

- [ ] Pythonæ¨ç†å®¹å™¨æˆåŠŸå¯åŠ¨
- [ ] æ‰€æœ‰7ä¸ªæ¨¡å‹æˆåŠŸä¸‹è½½åˆ° `/models/` ç›®å½•
- [ ] AIæœåŠ¡èƒ½å¤ŸæˆåŠŸè°ƒç”¨Pythonæ¨ç†
- [ ] E2Eæµ‹è¯•ä¸­æ‰€æœ‰AIæ¨¡å‹æµ‹è¯•é€šè¿‡
- [ ] æ¨ç†å“åº”æ—¶é—´ < 5ç§’
- [ ] æ¨ç†ç»“æœæ ¼å¼æ­£ç¡®ä¸”éç©º
- [ ] ç³»ç»Ÿå†…å­˜å ç”¨åˆç†ï¼ˆ< 8GBï¼‰

## ğŸ“ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. æ¨¡å‹é¢„åŠ è½½
åœ¨Pythonæ¨ç†æœåŠ¡å¯åŠ¨æ—¶é¢„åŠ è½½æ‰€æœ‰æ¨¡å‹åˆ°å†…å­˜ï¼š

```python
# åœ¨ inference_server.py ä¸­æ·»åŠ 
if __name__ == "__main__":
    # é¢„åŠ è½½æ‰€æœ‰æ¨¡å‹
    for model_type in MODEL_PATHS.keys():
        try:
            load_model(model_type)
            logger.info(f"Preloaded: {model_type}")
        except Exception as e:
            logger.error(f"Failed to preload {model_type}: {e}")
```

### 2. ä½¿ç”¨HTTPæœåŠ¡
å°†Pythonæ¨ç†æ”¹ä¸ºHTTPæœåŠ¡ï¼Œé¿å…æ¯æ¬¡éƒ½å¯åŠ¨æ–°è¿›ç¨‹ï¼š

```python
from flask import Flask, request, jsonify

app = Flask(__name__)

@app.route('/inference/<task_type>', methods=['POST'])
def inference(task_type):
    data = request.json
    result = process_inference(task_type, data)
    return jsonify(result)

if __name__ == "__main__":
    app.run(host='0.0.0.0', port=8085)
```

### 3. æ‰¹å¤„ç†
æ”¯æŒæ‰¹é‡æ¨ç†ä»¥æé«˜ååé‡ã€‚

### 4. æ¨¡å‹é‡åŒ–
ä½¿ç”¨FP16æˆ–INT8é‡åŒ–å‡å°‘å†…å­˜å ç”¨å’Œæé«˜é€Ÿåº¦ã€‚

## ğŸ”— ç›¸å…³æ–‡ä»¶

- Pythonæ¨ç†æœåŠ¡ï¼š`backend/ai-service/Dockerfile.inference`
- æ¨ç†è„šæœ¬ï¼š`backend/ai-service/scripts/inference_server.py`
- æ¨¡å‹ä¸‹è½½è„šæœ¬ï¼š`backend/ai-service/scripts/download_all_models.sh`
- AIæœåŠ¡æ›´æ–°ï¼š`backend/ai-service/services/real_inference_service.go`
- Dockeré…ç½®ï¼š`docker-compose.yml`
- E2Eæµ‹è¯•ï¼š`backend/tests/e2e_integration_test.go`

## âš ï¸ é‡è¦æç¤º

1. **ä¸å…è®¸ä»»ä½•æ¨¡æ‹Ÿ**ï¼šæ‰€æœ‰æ¨ç†å¿…é¡»ä½¿ç”¨çœŸå®æ¨¡å‹ï¼Œä¸å…è®¸é™çº§åˆ°æ¨¡æ‹Ÿå“åº”
2. **æ¨¡å‹å¿…é¡»ä¸‹è½½**ï¼šåœ¨è¿è¡ŒE2Eæµ‹è¯•å‰ï¼Œç¡®ä¿æ‰€æœ‰æ¨¡å‹å·²æˆåŠŸä¸‹è½½
3. **GPUæ¨è**ï¼šè™½ç„¶å¯ä»¥ä½¿ç”¨CPUï¼Œä½†GPUä¼šæ˜¾è‘—æé«˜æ¨ç†é€Ÿåº¦
4. **å†…å­˜è¦æ±‚**ï¼šå»ºè®®è‡³å°‘8GB RAM + 4GB Swap
5. **ç£ç›˜ç©ºé—´**ï¼šéœ€è¦è‡³å°‘2GBç©ºé—´å­˜å‚¨æ¨¡å‹

## ğŸ“ ä¸‹ä¸€æ­¥

æ‰§è¡Œä¸Šè¿°æ­¥éª¤åï¼Œè¿è¡ŒE2Eæµ‹è¯•éªŒè¯æ‰€æœ‰åŠŸèƒ½ï¼š

```bash
cd meeting-system/backend/tests
go test -v -run TestE2EIntegration 2>&1 | tee /tmp/e2e_with_real_models.log
```

é¢„æœŸæ‰€æœ‰AIæ¨¡å‹æµ‹è¯•æˆåŠŸç‡è¾¾åˆ°100%ï¼
